implement train.py script:

python train.py --config config.yaml

it load configuration variables from config.yaml file.

It will do these:

create SingleStockDataset (a class in dataset_loader.py) for each stock symbol listed in config.yaml "stock_symbols".

create a pytorch dataset class. In this class, each mini-batch sample will return "train_batch_size" data instances. each data instance is sampled in this way:
- first, randomly sampled one stock symbol S.
- for S, run the S.get_sample() and store the sample as X_dict
- use "num_workers" in config.yaml to sample data in parallel (multi-process)
- merge each "sample_data" as a big torch tensor "X".
- for each future_price, run extract_price_trend, get its "k", "b", "std". merge each data's ["k", "b", "std"] as a torch tensor Y of shape [batch_size, 3]

Now we get training data "X" and the training target "Y".  "X" is of shape [batch_size, history_window_size].

compute X_agg by averaging X along the history_window_size dimension for every non-overlapping window of size agg_windows_size.  For example, if history_window_size=360 , agg_windows_size=30, then X_agg is of shape (batch_size, history_window_size// agg_windows_size). Verify that history_window_size can be divided by agg_windows_size, otherwise raise error and terminate.

Now,  run mini-batch sgd to train a linea model Y= X_agg * W + b, where Y is of shape (batch_size,3), W is of shape (history_window_size// agg_windows_size, 3), b is of shape (1,3).

Use MSE loss function on Y.

use linear learning rate decay. See "lr", "weight_decay", "optimizer", "max_num_iters" in config.yaml for training setting.

Finally, save W and b as numpy array npz file in $output_dir/save_model/model.npz, with content:
{
"W", W,
"b": b
}